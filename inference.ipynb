{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference code for YOLOv5\n",
    "Offline evaluation pipeline\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_core import batch_inference\n",
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yaml   = 'data/bdd100k.yaml'\n",
    "weight = 'runs/train/exp11/weights/best.pt'\n",
    "batch_size = 32\n",
    "imgsz=640\n",
    "device='cuda:0'\n",
    "\n",
    "opt = {\n",
    "    \"conf_thres\": 0.001,\n",
    "    \"iou_thres\": 0.6,\n",
    "    \"augment\": True,\n",
    "    \"verbose\": False,\n",
    "    \"save_txt\": True,\n",
    "    \"save_conf\": False,\n",
    "    \"save_json\": False,\n",
    "    \"single_cls\": False,\n",
    "}\n",
    "\n",
    "opt = namedtuple(\"Opt\", opt.keys())(*opt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(data_yaml) as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "    \n",
    "nc = data['nc']  # number of classes\n",
    "iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "_ = model(img.half()) # run once\n",
    "path = data['test']\n",
    "dataloader = create_dataloader(path, imgsz, batch_size, model.stride.max(), opt, pad=0.5, rect=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "names = {k: v for k, v in enumerate(model.names)}\n",
    "s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "stats = []\n",
    "seen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "    img = img.to(device, non_blocking=True)\n",
    "    img = img.half() # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    targets = targets.to(device)\n",
    "    nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run model\n",
    "        t = time_synchronized()\n",
    "        output = make_inference(img, augment=opt.augment)  # inference\n",
    "        t0 += time_synchronized() - t\n",
    "        t1 += time_synchronized() - t\n",
    "        \n",
    "    # Statistics per image\n",
    "    for si, pred in enumerate(output):\n",
    "        labels = targets[targets[:, 0] == si, 1:] # get label of image si th\n",
    "        n_label = len(labels)\n",
    "        t_cls = labels[:, 0].tolist() if n_label else []  # target class\n",
    "        path = Path(paths[si])\n",
    "        seen += 1\n",
    "        \n",
    "        if len(pred) == 0:\n",
    "            if n_label:\n",
    "                stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), t_cls))\n",
    "            continue\n",
    "            \n",
    "        # Predictions\n",
    "        predn = pred.clone()\n",
    "        scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
    "        \n",
    "        \n",
    "        # Assign all predictions as incorrect\n",
    "        correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "        if nl:\n",
    "            detected = []  # target indices\n",
    "            tcls_tensor = labels[:, 0]\n",
    "\n",
    "            # target boxes\n",
    "            tbox = xywh2xyxy(labels[:, 1:5])\n",
    "            scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "\n",
    "            # Per target class\n",
    "            for cls in torch.unique(tcls_tensor):\n",
    "                ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                # Search for detections\n",
    "                if pi.shape[0]:\n",
    "                    # Prediction to target ious\n",
    "                    ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                    # Append detections\n",
    "                    detected_set = set()\n",
    "                    for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                        d = ti[i[j]]  # detected target\n",
    "                        if d.item() not in detected_set:\n",
    "                            detected_set.add(d.item())\n",
    "                            detected.append(d)\n",
    "                            correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                            if len(detected) == nl:  # all targets already located in image\n",
    "                                break\n",
    "\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), t_cls))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
